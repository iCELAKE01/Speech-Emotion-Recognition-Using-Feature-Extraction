# -*- coding: utf-8 -*-
"""T5_Emotion_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Kkn2FXB73o2j2NOOZLY4AcFg5ohtrK7
"""

import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer
import os # Import os for path checking

# Paths
model_dir = "/home/icelake2/T5Finetune/t5_finetuned_emotion/checkpoint-19815/"
transcription_file = "/home/icelake2/capstone_ASR/transcription_output.txt"
output_filename = "/home/icelake2/capstone_ASR/ASR_emotion_final.txt"
# --- Basic Checks (Optional but Recommended) ---
if not os.path.exists(model_dir):
    print(f"Error: Model directory not found at {model_dir}")
    exit()
if not os.path.exists(transcription_file):
    print(f"Error: Transcription file not found at {transcription_file}")
    exit()
# --- End Checks ---

# Read text
try:
    with open(transcription_file, "r", encoding="utf-8") as f:
        transcription_text = f.read().strip()
    if not transcription_text:
        print("Warning: Transcription file is empty.")
        # Handle empty file case if necessary, e.g., set default text or exit
        transcription_text = "Input text was empty." # Example handling
except Exception as e:
    print(f"Error reading transcription file: {e}")
    exit()


# Construct prompt - Use the desired final format directly in the prompt's instruction
# Changed "judge the emotion and respond in the format 'the speakers words convey [emotion]'"
# To clearly state the desired output prefix.
prompt = f"Given the following spoken text, determine the conveyed emotion.\nText: {transcription_text}"

print("Input Prompt:\n", prompt)

# Load tokenizer
try:
    # Ensure you are loading the correct tokenizer associated with your fine-tuned model
    tokenizer = T5Tokenizer.from_pretrained(model_dir, use_fast=False)
except Exception as e:
    print(f"Error loading tokenizer from {model_dir}: {e}")
    exit()

# Load model manually from safetensors
try:
    model = T5ForConditionalGeneration.from_pretrained(
        model_dir,
        torch_dtype=torch.float16, # Use float16 for faster inference if GPU supports it
        low_cpu_mem_usage=True     # Good practice for large models
    )
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval() # Set model to evaluation mode
    print(f"Model loaded successfully on {device}.")
except Exception as e:
    print(f"Error loading model from {model_dir}: {e}")
    exit()

# Encode input
inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device) # Added truncation

# Generate output
try:
    with torch.no_grad(): # Disable gradient calculations for inference
        output_ids = model.generate(
            **inputs,
            max_length=60,  # Increased slightly to allow for the full phrase + emotion
            num_beams=5,    # Keep beam search for potentially better results
            early_stopping=True # Stop generation early if EOS token is produced
            # Removed force_words_ids
        )

    # Decode output
    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    print("the speaker's words convey", output_text)

except Exception as e:
    print(f"Error during generation or decoding: {e}")
if output_text: # Only save if output was generated successfully
    try:
        with open(output_filename, "w", encoding="utf-8") as outfile:
            outfile.write("the speaker's words convey"+ " "+output_text)
        print(f"\nSuccessfully saved output to {output_filename}")
    except Exception as e:
        print(f"\nError writing output to file {output_filename}: {e}")
else:
    print("\nSkipping file save because output generation failed or was empty.")

